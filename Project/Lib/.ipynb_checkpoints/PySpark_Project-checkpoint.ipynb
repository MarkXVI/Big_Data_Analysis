{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee85da9",
   "metadata": {},
   "source": [
    "# PySpark_Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901ecb9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd6114d3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c14741",
   "metadata": {},
   "source": [
    "#### Checking Java version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58762df9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"19.0.2\" 2023-01-17\n",
      "Java(TM) SE Runtime Environment (build 19.0.2+7-44)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 19.0.2+7-44, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74898ab",
   "metadata": {},
   "source": [
    "#### Setting up Java environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6456e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Progra~1\\Java\\jdk-18.0.2.1\"\n",
    "os.environ[\"JAVA_PATH\"] = r\"C:\\Progra~1\\Java\\jdk-18.0.2.1\\bin\"\n",
    "os.environ[\"SPARK_HOME\"] = r\"D:\\School\\Big_Data_Analysis\\spark-3.3.2-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"D:\\School\\Big_Data_Analysis\\hadoop-3.3.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482efbf",
   "metadata": {},
   "source": [
    "### Spark\n",
    "#### Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e669687a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\mark9\\.conda\\envs\\ml-env\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: pyspark in c:\\users\\mark9\\.conda\\envs\\ml-env\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in c:\\users\\mark9\\.conda\\envs\\ml-env\\lib\\site-packages (from pyspark) (0.10.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f7f22",
   "metadata": {},
   "source": [
    "#### Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61907f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\School\\\\Big_Data_Analysis\\\\spark-3.3.2-bin-hadoop3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf91fa4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"NYC_Taxi_Analysis\") \\\n",
    "    .getOrCreate()\n",
    "#    .config(\"spark.driver.memory\", \"20g\") \\\n",
    "#    .config(\"spark.executor.memory\", \"20g\") \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6fd02",
   "metadata": {},
   "source": [
    "#### Stopping Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28bc6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a6968",
   "metadata": {},
   "source": [
    "#### Creating Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d71c4e49",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=NYC_Taxi_Analysis, master=local[*]) created by getOrCreate at C:\\Users\\mark9\\AppData\\Local\\Temp\\ipykernel_17256\\1437898141.py:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m conf\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m conf\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBD_Project\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(conf\u001b[38;5;241m=\u001b[39mconf)\n",
      "File \u001b[1;32m~\\.conda\\envs\\ML-ENV\\lib\\site-packages\\pyspark\\context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[1;32m~\\.conda\\envs\\ML-ENV\\lib\\site-packages\\pyspark\\context.py:350\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    347\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    353\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;241m%\u001b[39m (currentAppName, currentMaster,\n\u001b[0;32m    355\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction, callsite\u001b[38;5;241m.\u001b[39mfile, callsite\u001b[38;5;241m.\u001b[39mlinenum))\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    357\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=NYC_Taxi_Analysis, master=local[*]) created by getOrCreate at C:\\Users\\mark9\\AppData\\Local\\Temp\\ipykernel_17256\\1437898141.py:5 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf() # .setAll([('spark.driver.memory', '8g'), ('spark.executor.memory', '8g')]) # Set memory to 8GB\n",
    "conf.setMaster('local')\n",
    "conf.setAppName('BD_Project')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621d82da",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f17659",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f40613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read multiple CSV files into separate DataFrames\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/yellow_tripdata_2015-01.csv\")\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/yellow_tripdata_2016-01.csv\")\n",
    "df3 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/yellow_tripdata_2016-02.csv\")\n",
    "df4 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/yellow_tripdata_2016-03.csv\")\n",
    "\n",
    "# Combine the DataFrames into a single DataFrame\n",
    "taxi_df = df1.union(df2).union(df3).union(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44484cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|trip_distance|\n",
      "+-------------+\n",
      "|         1.59|\n",
      "|         3.30|\n",
      "|         1.80|\n",
      "|          .50|\n",
      "|         3.00|\n",
      "|         9.00|\n",
      "|         2.20|\n",
      "|          .80|\n",
      "|        18.20|\n",
      "|          .90|\n",
      "|          .90|\n",
      "|         1.10|\n",
      "|          .30|\n",
      "|         3.10|\n",
      "|         1.10|\n",
      "|         2.38|\n",
      "|         2.83|\n",
      "|         8.33|\n",
      "|         2.37|\n",
      "|         7.13|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the combined DataFrame\n",
    "#taxi_df.describe().show()\n",
    "taxi_df.select('trip_distance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "534d8a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- RateCodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10285bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|passenger_count|      avg_distance|\n",
      "+---------------+------------------+\n",
      "|              0|2.1291575359142922|\n",
      "|              1|  7.65459483526611|\n",
      "|              2| 9.357806903264088|\n",
      "|              3| 9.161549450371313|\n",
      "|              4| 5.992690105129649|\n",
      "|              5| 2.954346307893686|\n",
      "|              6| 2.878429682825396|\n",
      "|              7| 3.545061728395062|\n",
      "|              8| 5.225897435897437|\n",
      "|              9|4.7351470588235305|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Group the data by passenger count and calculate the average trip distance\n",
    "average_distance_by_passenger_count = (\n",
    "    taxi_df \\\n",
    "    .groupBy(\"passenger_count\")\n",
    "    .agg(avg(\"trip_distance\").alias(\"avg_distance\"))\n",
    "    .orderBy(\"passenger_count\")\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "average_distance_by_passenger_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f9e5b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+----------------------+--------------------+---------+\n",
      "|passenger_count|avg_distance_MILES|avg_euclidean_distance|avg_duration_minutes|num_trips|\n",
      "+---------------+------------------+----------------------+--------------------+---------+\n",
      "|              0|2.1291575359142922|     5.380060871682492|               18.04|     8214|\n",
      "|              1|  7.65459483526611|    0.3851992190090032|               14.82| 33537914|\n",
      "|              2| 9.357806903264088|    0.3460562562597229|               15.57|  6719430|\n",
      "|              3| 9.161549450371313|    0.3182944332218391|               15.92|  1912291|\n",
      "|              4| 5.992690105129649|   0.41554308932569695|               15.89|   911351|\n",
      "|              5| 2.954346307893686|   0.10893346292222877|               16.62|  2551660|\n",
      "|              6| 2.878429682825396|    0.1029063764571677|               16.36|  1607758|\n",
      "|              7| 3.545061728395062|     17.76185185185185|               11.46|       81|\n",
      "|              8| 5.225897435897437|      8.71474358974359|               30.51|       78|\n",
      "|              9|4.7351470588235305|    14.954264705882355|               13.06|       68|\n",
      "+---------------+------------------+----------------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, round, avg, sqrt, count\n",
    "\n",
    "# Filter out trips with invalid coordinates and passenger counts\n",
    "df = taxi_df.filter(col('pickup_longitude').isNotNull()) \\\n",
    "            .filter(col('pickup_latitude').isNotNull()) \\\n",
    "            .filter(col('dropoff_longitude').isNotNull()) \\\n",
    "            .filter(col('dropoff_latitude').isNotNull()) \\\n",
    "            .filter(col('trip_distance').isNotNull())\n",
    "\n",
    "# Calculate the trip distance and duration\n",
    "df = df.withColumn('trip_euclidean_distance', round(sqrt((col('dropoff_longitude') - col('pickup_longitude'))**2 + (col('dropoff_latitude') - col('pickup_latitude'))**2), 2)) \\\n",
    "       .withColumn('trip_duration', unix_timestamp(col(\"tpep_dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\") - unix_timestamp(col(\"tpep_pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Group the data by passenger count and calculate average trip distance\n",
    "average_distance_by_passenger_count = df.groupBy(\"passenger_count\") \\\n",
    "                                        .agg(avg(\"trip_distance\").alias(\"avg_distance_MILES\"))\n",
    "\n",
    "# Group the data by passenger count and calculate the average euclidean_distance\n",
    "average_euclidean_distance_by_passenger_count = df.groupBy('passenger_count') \\\n",
    "                                            .agg(avg('trip_euclidean_distance') \\\n",
    "                                            .alias('avg_euclidean_distance'), count('*').alias('num_trips'))\n",
    "\n",
    "# Group the data by passenger count and calculate the average trip time\n",
    "trip_duration_by_passenger_count = df.groupBy('passenger_count') \\\n",
    "                                     .agg(avg('trip_duration') \\\n",
    "                                     .alias('avg_duration'))\n",
    "\n",
    "# Join the two DataFrames on passenger count and calculate the average trip time in minutes\n",
    "result = average_euclidean_distance_by_passenger_count.join(trip_duration_by_passenger_count, 'passenger_count') \\\n",
    "            .join(average_distance_by_passenger_count, 'passenger_count') \\\n",
    "            .withColumn('avg_duration_minutes', round(col('avg_duration') / 60, 2)) \\\n",
    "            .select('passenger_count', 'avg_distance_MILES', 'avg_euclidean_distance', 'avg_duration_minutes', 'num_trips') \\\n",
    "            .orderBy('passenger_count')\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b9917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
